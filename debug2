# import os
# import pandas as pd
# from recbole.data import create_dataset
# from recbole.config import Config

# # ============================================
# # RECBOLE MAPPING DIAGNOSTIC
# # ============================================

# RECBOLE_DATA_PATH = "/scratch/jd5316/Capstone/Capstone/Data/recbole_data/"
# DATASET_NAME = "my_data_set"

# parameter_dict = {
#     "model": "LightGCN",
#     "dataset": DATASET_NAME,
#     "data_path": RECBOLE_DATA_PATH,
#     "field_separator": ",",
#     "USER_ID_FIELD": "user_id",
#     "ITEM_ID_FIELD": "item_id",
#     "load_col": {"inter": ["user_id", "item_id"]},
# }

# config = Config(model="LightGCN", dataset=DATASET_NAME, config_dict=parameter_dict)
# dataset = create_dataset(config)

# print("="*80)
# print("INSPECTING RECBOLE DATASET OBJECT")
# print("="*80)

# # Check what fields exist
# print("\n1. Dataset fields:")
# print(f"   uid_field: {dataset.uid_field}")
# print(f"   iid_field: {dataset.iid_field}")

# # Check field2id_token
# print("\n2. field2id_token contents:")
# print(f"   Type: {type(dataset.field2id_token)}")
# print(f"   Keys: {list(dataset.field2id_token.keys())}")

# if dataset.uid_field in dataset.field2id_token:
#     user_array = dataset.field2id_token[dataset.uid_field]
#     print(f"\n   User field2id_token:")
#     print(f"     Type: {type(user_array)}")
#     print(f"     Shape: {user_array.shape if hasattr(user_array, 'shape') else 'N/A'}")
#     print(f"     First 10 elements: {user_array[:10]}")
#     print(f"     Element at index 1: '{user_array[1]}' (type: {type(user_array[1])})")
#     print(f"     Element at index 2: '{user_array[2]}' (type: {type(user_array[2])})")

# if dataset.iid_field in dataset.field2id_token:
#     item_array = dataset.field2id_token[dataset.iid_field]
#     print(f"\n   Item field2id_token:")
#     print(f"     Type: {type(item_array)}")
#     print(f"     Shape: {item_array.shape if hasattr(item_array, 'shape') else 'N/A'}")
#     print(f"     First 10 elements: {item_array[:10]}")
#     print(f"     Element at index 1: '{item_array[1]}' (type: {type(item_array[1])})")

# # Check field2token_id
# print("\n3. field2token_id contents:")
# print(f"   Type: {type(dataset.field2token_id)}")
# print(f"   Keys: {list(dataset.field2token_id.keys())}")

# if dataset.uid_field in dataset.field2token_id:
#     user_dict = dataset.field2token_id[dataset.uid_field]
#     print(f"\n   User field2token_id:")
#     print(f"     Type: {type(user_dict)}")
#     print(f"     Size: {len(user_dict)}")
#     print(f"     First 5 items:")
#     for i, (token, idx) in enumerate(list(user_dict.items())[:5]):
#         print(f"       '{token}' â†’ {idx}")

# if dataset.iid_field in dataset.field2token_id:
#     item_dict = dataset.field2token_id[dataset.iid_field]
#     print(f"\n   Item field2token_id:")
#     print(f"     Type: {type(item_dict)}")
#     print(f"     Size: {len(item_dict)}")
#     print(f"     First 5 items:")
#     for i, (token, idx) in enumerate(list(item_dict.items())[:5]):
#         print(f"       '{token}' â†’ {idx}")

# # Test id2token method
# print("\n4. Testing id2token method:")
# try:
#     token_1 = dataset.id2token(dataset.uid_field, 1)
#     token_2 = dataset.id2token(dataset.uid_field, 2)
#     print(f"   id2token(user, 1) = '{token_1}' (type: {type(token_1)})")
#     print(f"   id2token(user, 2) = '{token_2}' (type: {type(token_2)})")
# except Exception as e:
#     print(f"   Error calling id2token: {e}")

# # Test with batch
# try:
#     tokens_batch = dataset.id2token(dataset.uid_field, [1, 2, 3])
#     print(f"\n   id2token(user, [1,2,3]) = {tokens_batch}")
#     print(f"   Type: {type(tokens_batch)}")
#     if hasattr(tokens_batch, '__iter__'):
#         print(f"   Elements: {[str(t) for t in tokens_batch]}")
# except Exception as e:
#     print(f"   Error calling id2token with batch: {e}")

# # Check the actual CSV files to see what tokens look like
# print("\n5. Checking original CSV tokens:")
# train_inter_path = os.path.join(RECBOLE_DATA_PATH, DATASET_NAME, "train.inter")
# if os.path.exists(train_inter_path):
#     df = pd.read_csv(train_inter_path, nrows=5)
#     print(f"   Train.inter sample:")
#     print(df)
#     print(f"\n   First user token: '{df.iloc[0, 0]}'")
#     print(f"   First item token: '{df.iloc[0, 1]}'")

# print("\n" + "="*80)
# print("DIAGNOSTIC COMPLETE")
# print("="*80)

# import pandas as pd

# # ====================================================
# # CONFIG â€” EDIT THESE PATHS
# # ====================================================
# TEST_PATH      = "/scratch/jd5316/Capstone/Capstone/results/lightgcn_base/test_internal.csv"
# REC_PATH       = "/scratch/jd5316/Capstone/Capstone/results/lightgcn_base/lightgcn_topk.parquet"
# ITEM_PATH      = "/scratch/jd5316/Capstone/Capstone/Data/sampled/sampled_music_info.csv"
# # ====================================================


# print("\n=======================================================")
# print("ğŸ” LOADING FILES")
# print("=======================================================\n")

# test_df = pd.read_csv(TEST_PATH)
# rec_df  = pd.read_parquet(REC_PATH)
# item_df = pd.read_csv(ITEM_PATH)

# print(f"âœ“ test_df shape = {test_df.shape}")
# print(f"âœ“ rec_df shape  = {rec_df.shape}")
# print(f"âœ“ item_df shape = {item_df.shape}")

# print("\n=======================================================")
# print("ğŸ” SAMPLE ROWS")
# print("=======================================================\n")

# print("Test set example row:")
# print(test_df.head(1), "\n")

# print("Recommendation example row:")
# print(rec_df.head(1), "\n")

# print("Item feature example row:")
# print(item_df.head(1), "\n")


# # ====================================================
# # PRINT COLUMN NAMES
# # ====================================================
# print("\n=======================================================")
# print("ğŸ” COLUMN NAMES")
# print("=======================================================\n")

# print("test_df columns:", list(test_df.columns))
# print("rec_df columns:",  list(rec_df.columns))
# print("item_df columns:", list(item_df.columns))


# # ====================================================
# # CHECK TYPES
# # ====================================================
# print("\n=======================================================")
# print("ğŸ” DATA TYPES")
# print("=======================================================\n")

# print(test_df.dtypes)
# print(rec_df.dtypes)
# print(item_df.dtypes)


# # ====================================================
# # CHECK ID OVERLAP
# # ====================================================
# print("\n=======================================================")
# print("ğŸ” USER ID OVERLAP CHECK")
# print("=======================================================\n")

# test_users = set(test_df["user_id"].astype(str))
# rec_users  = set(rec_df["user_id"].astype(str))

# overlap_users = test_users & rec_users

# print(f"Test users: {len(test_users)}")
# print(f"Rec users:  {len(rec_users)}")
# print(f"Overlap:    {len(overlap_users)} ({len(overlap_users)/max(1,len(test_users))*100:.2f}%)")


# print("\n=======================================================")
# print("ğŸ” ITEM ID OVERLAP CHECK")
# print("=======================================================\n")

# test_items = set(test_df["track_id"].astype(str))
# rec_items  = set(rec_df["item_id"].astype(str))
# feat_items = set(item_df.index.astype(str)) if item_df.index.name else set(item_df["track_id"].astype(str))

# overlap_test_rec = test_items & rec_items
# overlap_rec_feat = rec_items & feat_items

# print(f"Test items: {len(test_items)}")
# print(f"Rec items:  {len(rec_items)}")
# print(f"Feature items: {len(feat_items)}")

# print(f"Overlap (test âˆ© rec): {len(overlap_test_rec)} ({len(overlap_test_rec)/max(1,len(test_items))*100:.2f}%)")
# print(f"Overlap (rec âˆ© features): {len(overlap_rec_feat)} ({len(overlap_rec_feat)/max(1,len(rec_items))*100:.2f}%)")


# # ====================================================
# # SHOW EXACT MISMATCH EXAMPLES
# # ====================================================
# print("\n=======================================================")
# print("ğŸ” EXACT MISMATCH EXAMPLES")
# print("=======================================================\n")

# missing_in_rec = list(test_items - rec_items)[:10]
# missing_in_features = list(rec_items - feat_items)[:10]

# if missing_in_rec:
#     print("Items in TEST but not in REC (first 10):")
#     print(missing_in_rec)
# else:
#     print("âœ“ All test items appear in rec_df!")

# if missing_in_features:
#     print("\nItems in REC but not in FEATURE DF (first 10):")
#     print(missing_in_features)
# else:
#     print("âœ“ All recommended items appear in item_df!")


# # ====================================================
# # STRING CLEANING CHECKS
# # ====================================================
# print("\n=======================================================")
# print("ğŸ” STRING FORMAT CHECK â€” look for hidden characters")
# print("=======================================================\n")

# def inspect_id(id_value):
#     return id_value, repr(id_value), len(str(id_value))

# sample_rec_id = str(rec_df["item_id"].iloc[0])
# sample_test_id = str(test_df["track_id"].iloc[0])
# sample_feat_id = str(list(feat_items)[0])

# print("Example rec_df item_id:", inspect_id(sample_rec_id))
# print("Example test_df track_id:", inspect_id(sample_test_id))
# print("Example item_df track_id:", inspect_id(sample_feat_id))

# ///////////////////////////////////////////////////////////////

# import json
# with open("/scratch/jd5316/Capstone/Capstone/results/lightgcn_base/id_mappings.json") as f:
#     mapping = json.load(f)
# len_item_map = len(mapping.get("id2item", {}))
# len_user_map = len(mapping.get("id2user", {}))
# print("id2item entries:", len_item_map)
# print("id2user entries:", len_user_map)

# import pandas as pd
# train = pd.read_csv("/scratch/jd5316/Capstone/Capstone/results/lightgcn_base/train_internal.csv")
# print("unique train item IDs:", train["track_id"].astype(str).nunique())
# print("unique train user IDs:", train["user_id"].astype(str).nunique())

# /////////////////////////////////////////////////////////////////////

import os
import pandas as pd
import numpy as np

ROOT = "/scratch/jd5316/Capstone/Capstone/results/"   # change if needed
EXT = ".parquet"

def analyze_topk(path):
    print("\n" + "="*100)
    print(f"ğŸ” Checking file: {path}")

    try:
        df = pd.read_parquet(path)
    except Exception as e:
        print(f"âŒ Failed to load: {e}")
        return

    # Basic stats
    n_users = df["user_id"].nunique()
    n_items = df["item_id"].nunique()
    n_rows = len(df)

    print(f"Rows: {n_rows}")
    print(f"Unique users: {n_users}")
    print(f"Unique items: {n_items}")

    # Detect per-user K
    k_values = df.groupby("user_id")["rank"].max().unique()
    if len(k_values) == 1:
        K = k_values[0]
        print(f"K per user: {K}")
    else:
        print(f"âš ï¸ Multiple K values: {sorted(k_values)[:10]}")

    # Score distribution
    print(f"Score range: {df['score'].min():.6f} â†’ {df['score'].max():.6f}")

    # ------------------------------------------------------
    # Classify file: baseline or reranker?
    # ------------------------------------------------------
    suspicious = False

    if n_items > 20000:
        print("ğŸŸ¢ Looks like a full-universe baseline file (20k+ items).")
    elif n_items < 12000:
        print("ğŸŸ¡ Likely reranker output (few thousand items).")
        suspicious = True
    else:
        print("ğŸŸ  Unclear universe size â€” borderline.")

    # Check for users not receiving full K
    expected_rows = n_users * (k_values[0] if len(k_values)==1 else 100)
    if n_rows < expected_rows:
        print(f"âš ï¸ Missing recommendations: expected ~{expected_rows}, got {n_rows}")
        suspicious = True

    if suspicious:
        print("ğŸš¨ WARNING: This file may be causing corrupted evaluation.")

    print("="*100)


def scan_directory(root):
    print(f"Scanning for .parquet files under: {root}\n")

    for dirpath, _, filenames in os.walk(root):
        for f in filenames:
            if f.endswith(EXT):
                analyze_topk(os.path.join(dirpath, f))


if __name__ == "__main__":
    scan_directory(ROOT)
